import tensorflow as tf

# Constants
IMG_SIZE = 224
BATCH_SIZE = 32
NUM_CLASSES = 10
EPOCHS = 5

# Load CIFAR-10 dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()

# Convert labels to one-hot encoding
y_train = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)
y_test = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)

# Create tf.data.Dataset objects for training and testing
train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))
test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))

# Preprocessing function: resize and preprocess input for EfficientNet
def preprocess(image, label):
    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))
    image = tf.keras.applications.efficientnet.preprocess_input(image)
    return image, label

# Apply preprocessing, batch and prefetch
train_ds = train_ds.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
test_ds = test_ds.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

# Load EfficientNetB0 pretrained model without top classifier layer
base_model = tf.keras.applications.EfficientNetB0(
    weights='imagenet',
    include_top=False,
    input_shape=(IMG_SIZE, IMG_SIZE, 3)
)
base_model.trainable = False  # Freeze base model layers

# Build classification model
model = tf.keras.Sequential([
    base_model,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')
])

# Compile model
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Show model summary
model.summary()

# Train the model
model.fit(
    train_ds,
    epochs=EPOCHS,
    validation_data=test_ds
)

# Evaluate model on test set
loss, accuracy = model.evaluate(test_ds)
print(f"Test accuracy: {accuracy:.4f}")



This code performs image classification on the CIFAR-10 dataset using transfer learning with the EfficientNetB0 model. CIFAR-10 contains 60,000 color images (32×32 pixels) across 10 classes. The labels are one-hot encoded, and the dataset is converted into tf.data.Dataset objects for efficient preprocessing and training. Each image is resized to 224×224 to match EfficientNet's input requirements and preprocessed using its specific normalization function. The base model, EfficientNetB0 (pretrained on ImageNet), is loaded without its top classification layers and is frozen to retain its learned features. A custom classification head is added, consisting of global average pooling and a dense layer with softmax activation to output probabilities across 10 classes.

The model is compiled using the Adam optimizer and categorical crossentropy loss, suitable for multi-class classification with one-hot labels. It is trained for 5 epochs and evaluated on the test set to report accuracy. By leveraging EfficientNet’s pretrained convolutional base, this approach enables fast and effective training with high accuracy, even on small datasets, demonstrating the power of transfer learning for image-based deep learning tasks.