# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Step 1: Load the dataset
dataset = pd.read_csv("Mall_Customers.csv") 
# Display basic dataset information
print("\nDataset Info:\n", dataset.info())

# Display the first five rows of the dataset
print("\nFirst 5 rows of dataset:\n", dataset.head())

# Step 2: Selecting features for clustering
X = dataset.iloc[:, [3, 4]].values  # Extracting 'Annual Income' and 'Spending Score'

# Display the first few rows
print("\nFirst 5 rows of selected features:\n", X[:5])

# Step 3: Using the Elbow Method to determine the optimal number of clusters
wcss = []  # List to store Within-Cluster Sum of Squares

# Testing k values from 1 to 10
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)  # Append WCSS value

# Plot the Elbow Method graph
plt.figure(figsize=(8, 5))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--', color='b')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.grid()
plt.show()

# Step 4: Apply K-Means with optimal k (Assume k=5 based on Elbow Method)
optimal_k = 5  # Change this if a different k value is found from the elbow method

kmeans = KMeans(n_clusters=optimal_k, init='k-means++', random_state=42)
y_kmeans = kmeans.fit_predict(X)  # Fit the model and predict clusters

# Add cluster labels to dataset
dataset['Cluster'] = y_kmeans

# Display the first few rows with cluster numbers
print("\nDataset with cluster assignments:\n", dataset.head())

# Step 5: Visualizing the Clusters
plt.figure(figsize=(8, 6))

# Scatter plot for each cluster
plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s=100, c='red', label='Cluster 1')
plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s=100, c='blue', label='Cluster 2')
plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s=100, c='green', label='Cluster 3')
plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s=100, c='cyan', label='Cluster 4')
plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s=100, c='magenta', label='Cluster 5')

# Plotting centroids
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], 
            s=300, c='yellow', label='Centroids', edgecolors='black')

# Titles and Labels
plt.title('Customer Segmentation using K-Means')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.grid()
plt.show()






K-Means clustering is an unsupervised machine learning algorithm widely used for data segmentation, particularly in applications such as customer classification and market analysis. In this project, the goal is to segment mall customers based on two features: Annual Income and Spending Score, using data from the Mall_Customers.csv file. These two numerical attributes were selected because they can help differentiate customer spending behaviors, allowing businesses to better understand and target various customer groups.

The process begins with loading and examining the dataset using the Pandas library. From the dataset, only the columns related to annual income and spending score are extracted, forming a 2D feature array that will be used for clustering. Before applying the clustering algorithm, it is crucial to determine the optimal number of clusters. This is done using the Elbow Method, where the K-Means algorithm is applied repeatedly with cluster counts ranging from 1 to 10. For each value of k, the Within-Cluster Sum of Squares (WCSS) is calculated — a measure of how compact the clusters are. The WCSS values are plotted against the number of clusters, and the "elbow" point on this graph, where the rate of decrease sharply changes, is considered the best choice for k. In this case, the elbow point was assumed to be at k = 5.

Once the optimal number of clusters is determined, the K-Means algorithm is applied with k = 5. The init='k-means++' parameter ensures smart initialization of centroids, which leads to faster convergence and more stable results. The algorithm iteratively assigns each data point to the nearest centroid and then recalculates the centroids based on the new cluster assignments. This process continues until the positions of the centroids stabilize and the model converges. The predicted cluster labels for each customer are then added to the original dataset for reference.

To better understand and communicate the results, the clustered data is visualized using a scatter plot. Each cluster is plotted in a different color, and the computed centroids are highlighted in yellow. This visualization helps interpret the customer segments visually — for example, identifying groups such as high-income, high-spending customers or low-income, low-spending customers. Such insights are extremely valuable for targeted marketing, personalized recommendations, and resource allocation.

Overall, this code demonstrates a complete pipeline for customer segmentation using K-Means clustering, starting from data loading, feature selection, determining the optimal number of clusters using the Elbow Method, applying the algorithm, and finally visualizing the segmented results. It illustrates important machine learning concepts such as unsupervised learning, WCSS, clustering, centroid-based partitioning, and the power of visual analytics in business decision-making.